"""
Sandbox pool implementation for efficient sandbox reuse and management.

Provides a producer-consumer pattern where:
- Producer thread creates sandboxes in batches in the background
- Main loop acquires pre-warmed sandboxes instantly
- Sandboxes are cleaned and returned to pool for reuse
"""

import asyncio
import logging
import queue
import threading
import time
import traceback

from prime_sandboxes import AsyncSandboxClient, CreateSandboxRequest


class SandboxPool:
    """
    Producer-consumer pool for pre-created sandboxes.

    Architecture:
    - Producer thread: Separate thread + event loop, creates sandboxes in parallel
    - Main event loop: Orchestrates rollouts, manages pool
    - Worker threads (100): Each has own event loop + AsyncSandboxClient for test execution

    Pool sizing: Size based on concurrent TEST EXECUTIONS, not total rollouts.
    Since sandboxes are only acquired during test execution (5-15s), you can support
    many more rollouts than sandboxes. E.g., with 1000 sandboxes and 10s test time,
    you can support ~10,000+ concurrent rollouts depending on LLM latency.

    CRITICAL INVARIANTS (enforced with defensive checks):
    1. len(all_sandboxes) + pending_creates <= pool_size (never over-create)
    2. ready_queue.qsize() <= pool_size (never overfill queue)
    3. pending_creates >= 0 (accounting must be correct)
    4. in_use_sandboxes âŠ† all_sandboxes (only track known sandboxes)
    5. ready + in_use ~= total (all sandboxes accounted for)

    Cleanup: Disabled for performance. Bundles use unique names (UUIDs) so no conflicts on reuse.
    """

    def __init__(
        self,
        sandbox_client: AsyncSandboxClient,
        sandbox_request: CreateSandboxRequest,
        pool_size: int = 10,
        max_concurrent_creates: int = 100,
        timeout_minutes: int = 360,
    ):
        self.sandbox_client = sandbox_client
        self.sandbox_request = sandbox_request
        self.pool_size = pool_size
        self.max_concurrent_creates = max_concurrent_creates
        self.timeout_minutes = timeout_minutes
        self.timeout_seconds = timeout_minutes * 60

        # Thread-safe queue for communication between threads
        self.ready_queue: queue.Queue[str] = queue.Queue(maxsize=pool_size)

        # Track all sandboxes (thread-safe using locks)
        self._lock = threading.Lock()
        self.all_sandboxes: set[str] = set()
        self.in_use_sandboxes: set[str] = set()
        self.sandbox_creation_times: dict[str, float] = {}
        self.pending_creates: int = 0

        # Rate limit "waiting for sandbox" log spam
        self._last_waiting_log = 0.0

        # Producer thread and its event loop
        self.producer_thread: threading.Thread | None = None
        self.producer_loop: asyncio.AbstractEventLoop | None = None
        self.shutdown_event = threading.Event()
        self._started = False
        self._start_lock = threading.Lock()

        # Semaphore for producer's own event loop
        self.producer_semaphore: asyncio.Semaphore | None = None

        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    async def start(self):
        """Start the producer thread (idempotent)."""
        with self._start_lock:
            if self._started:
                return

            self.logger.info(f"Starting sandbox pool producer thread (pool_size={self.pool_size})")
            self.producer_thread = threading.Thread(
                target=self._run_producer_thread,
                daemon=True,
                name="SandboxPoolProducer",
            )
            self.producer_thread.start()
            self._started = True

    def _run_producer_thread(self):
        """
        Entry point for producer thread.
        Creates its own event loop and AsyncSandboxClient for this thread.
        """
        try:
            # Create new event loop for this thread
            self.producer_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.producer_loop)

            # Create producer's own AsyncSandboxClient bound to this event loop
            # This avoids "bound to different event loop" errors
            # Limit connections based on max_concurrent_creates (50)
            self.producer_client = AsyncSandboxClient(
                max_connections=self.max_concurrent_creates * 6,  # 300 for aggressive creation
                max_keepalive_connections=self.max_concurrent_creates * 3,  # 150 keepalive
            )

            # Create semaphore in this thread's event loop
            self.producer_semaphore = asyncio.Semaphore(self.max_concurrent_creates)

            # Run the producer loop
            self.logger.debug("Producer thread started with dedicated event loop and client")
            self.producer_loop.run_until_complete(self._producer_loop())
        except Exception as e:
            self.logger.error(f"Producer thread crashed: {repr(e)}")
            traceback.print_exc()
        finally:
            if self.producer_loop:
                self.producer_loop.close()
            self.logger.debug("Producer thread exiting")

    async def _producer_loop(self):
        """
        Continuously create sandboxes to maintain pool size.
        Runs in dedicated thread with its own event loop.
        """
        last_pool_status_log = 0.0
        pool_status_log_interval = 5.0  # Log pool status every 5 seconds for visibility

        while not self.shutdown_event.is_set():
            try:
                # Calculate how many sandboxes we need to create (thread-safe)
                with self._lock:
                    total_sandboxes = len(self.all_sandboxes)
                    in_use = len(self.in_use_sandboxes)
                    pending = self.pending_creates
                    # Include pending creates to prevent over-creation
                    effective_total = total_sandboxes + pending

                ready_count = self.ready_queue.qsize()
                needed = self.pool_size - effective_total

                # Log pool status regularly for debugging
                current_time = time.time()
                if current_time - last_pool_status_log >= pool_status_log_interval:
                    status_parts = [
                        f"{ready_count} ready",
                        f"{in_use} in-use",
                        f"{total_sandboxes}/{self.pool_size} total",
                    ]
                    if pending > 0:
                        status_parts.append(f"{pending} preparing")
                    if needed > 0:
                        status_parts.append(f"need {needed} more")

                    self.logger.debug(f"Pool: {', '.join(status_parts)}")
                    last_pool_status_log = current_time

                if needed > 0:
                    # Create sandboxes in parallel batches
                    current_batch_size = min(needed, self.max_concurrent_creates)

                    # Reserve capacity before creating
                    with self._lock:
                        self.pending_creates += current_batch_size

                    self.logger.debug(f"Producer: Creating batch of {current_batch_size} sandboxes...")

                    # Create batch of sandboxes concurrently using producer's own semaphore
                    batch_start = time.perf_counter()

                    # First, create all sandboxes in parallel
                    create_tasks = [self._create_sandbox() for _ in range(current_batch_size)]
                    create_results = await asyncio.gather(*create_tasks, return_exceptions=True)

                    # Collect successful sandbox IDs
                    pending_sandbox_ids = []
                    for result in create_results:
                        if isinstance(result, Exception):
                            self.logger.error(f"Producer: error creating sandbox: {repr(result)}")
                        elif result is not None:
                            pending_sandbox_ids.append(result)

                    # Wait for them to become RUNNING (they're added to pool inside the wait function)
                    if pending_sandbox_ids:
                        ready_sandbox_ids = await self._wait_for_sandboxes_running_batch(
                            pending_sandbox_ids, timeout=600.0
                        )

                        # Release capacity for failed creates
                        failed = current_batch_size - len(ready_sandbox_ids)
                        if failed > 0:
                            with self._lock:
                                self.pending_creates -= failed

                        successful = len(ready_sandbox_ids)
                        batch_time = time.perf_counter() - batch_start
                        if successful > 0:
                            self.logger.debug(
                                f"Batch complete: {successful}/{current_batch_size} sandboxes ready in {batch_time:.2f}s "
                                f"({batch_time / successful:.2f}s avg)"
                            )

                    # CRITICAL: Verify invariants after batch
                    with self._lock:
                        if self.pending_creates < 0:
                            self.logger.error(
                                f"CRITICAL: pending_creates is negative ({self.pending_creates}), resetting to 0"
                            )
                            self.pending_creates = 0
                        if len(self.all_sandboxes) > self.pool_size:
                            self.logger.error(
                                f"CRITICAL: all_sandboxes exceeds pool_size ({len(self.all_sandboxes)} > {self.pool_size})"
                            )
                        # Verify accounting: in_use should be subset of all_sandboxes
                        if len(self.in_use_sandboxes) > len(self.all_sandboxes):
                            self.logger.error(
                                f"CRITICAL: in_use sandboxes ({len(self.in_use_sandboxes)}) exceeds total ({len(self.all_sandboxes)})"
                            )
                else:
                    # Pool is full, wait before checking again
                    await asyncio.sleep(1.0)

            except Exception as e:
                self.logger.error(f"Producer: error in loop: {repr(e)}")
                traceback.print_exc()
                await asyncio.sleep(5.0)  # Back off on error

        self.logger.debug("Producer loop exiting")

    async def _create_sandbox(self) -> str:
        """
        Create a single sandbox (does not wait for RUNNING).
        Uses producer's own semaphore for concurrency control.
        """
        # Check if shutdown was requested before creating
        if self.shutdown_event.is_set():
            raise RuntimeError("Shutdown requested, aborting sandbox creation")

        # Use producer's own semaphore (not shared with rollouts)
        assert self.producer_semaphore is not None
        async with self.producer_semaphore:
            # Create sandbox using producer's own client
            sandbox = await self.producer_client.create(self.sandbox_request)
            return sandbox.id

    async def _get_sandbox_statuses(self) -> dict[str, str]:
        """
        Get status for all sandboxes using list() API.
        More efficient than individual GET calls when checking multiple sandboxes.
        """
        try:
            sandboxes = []
            page = 1
            list_start = time.perf_counter()

            while True:
                list_response = await asyncio.wait_for(
                    self.producer_client.list(
                        team_id=self.sandbox_request.team_id,
                        per_page=1000,
                        page=page,
                        exclude_terminated=True,
                    ),
                    timeout=10.0,  # Reduced from 30s - fail faster if API struggling
                )
                sandboxes.extend(list_response.sandboxes)

                if not getattr(list_response, "has_next", False):
                    break
                page += 1

            list_time = time.perf_counter() - list_start
            if list_time > 10.0:
                self.logger.warning(f"Slow list() API: {list_time:.1f}s to fetch {len(sandboxes)} sandboxes")
            return {sb.id: sb.status for sb in sandboxes}
        except asyncio.TimeoutError:
            self.logger.error("Timeout listing sandboxes after 10s - API overloaded!")
            return {}
        except Exception as e:
            self.logger.warning(f"Error listing sandboxes: {repr(e)}")
            return {}

    async def _wait_for_sandboxes_running_batch(self, sandbox_ids: list[str], timeout: float = 600.0) -> list[str]:
        """
        Wait for multiple sandboxes to reach RUNNING status using batch list() API.
        More efficient than waiting for each individually.

        Returns list of sandbox IDs that became RUNNING.
        """
        start_time = time.perf_counter()
        pending = set(sandbox_ids)
        ready = []

        while pending:
            # Check if shutdown was requested
            if self.shutdown_event.is_set():
                raise RuntimeError("Shutdown requested while waiting for sandboxes")

            elapsed = time.perf_counter() - start_time
            if elapsed > timeout:
                self.logger.warning(
                    f"Timeout waiting for {len(pending)} sandboxes: {pending}. "
                    f"Returning {len(ready)} that became ready."
                )
                break

            # Single list() call checks all pending sandboxes at once
            statuses = await self._get_sandbox_statuses()

            # Check which ones are RUNNING and add them to pool immediately
            newly_ready = []
            for sandbox_id in list(pending):
                status = statuses.get(sandbox_id)
                if status == "RUNNING":
                    newly_ready.append(sandbox_id)
                    pending.remove(sandbox_id)
                    ready.append(sandbox_id)

                    with self._lock:
                        self.all_sandboxes.add(sandbox_id)
                        self.sandbox_creation_times[sandbox_id] = time.time()
                        self.pending_creates -= 1
                    self.ready_queue.put(sandbox_id)

            if newly_ready:
                self.logger.debug(f"Added {len(newly_ready)} sandboxes to pool, {len(pending)} still preparing")

            if pending:
                await asyncio.sleep(1.0)

        return ready

    async def acquire(self, timeout: float | None = None) -> str:
        """
        Acquire a sandbox from the pool.
        Called from main event loop, uses executor to handle blocking queue.get().

        Args:
            timeout: Maximum time to wait for a sandbox (None = wait forever)

        Returns:
            Sandbox ID
        """
        loop = asyncio.get_running_loop()

        ready_before = self.ready_queue.qsize()
        if ready_before == 0:
            current_time = time.time()
            if current_time - self._last_waiting_log >= 5.0:
                with self._lock:
                    in_use = len(self.in_use_sandboxes)
                    pending = self.pending_creates
                total = len(self.all_sandboxes)
                self.logger.warning(
                    f"Pool exhausted! 0 ready, {in_use} in-use, {total} total, "
                    f"{pending} preparing - rollouts are waiting"
                )
                self._last_waiting_log = current_time

        try:
            while True:
                if timeout is not None:
                    sandbox_id = await asyncio.wait_for(
                        loop.run_in_executor(None, self.ready_queue.get), timeout=timeout
                    )
                else:
                    sandbox_id = await loop.run_in_executor(None, self.ready_queue.get)

                with self._lock:
                    creation_time = self.sandbox_creation_times.get(sandbox_id)

                if creation_time:
                    age_seconds = time.time() - creation_time
                    remaining_seconds = self.timeout_seconds - age_seconds
                    age_minutes = age_seconds / 60

                    if remaining_seconds < self.timeout_seconds * 0.1:
                        self.logger.warning(
                            f"Sandbox {sandbox_id} too old (age: {age_minutes:.1f}m, {remaining_seconds / 60:.1f}m remaining), removing from pool"
                        )
                        await self.remove(sandbox_id)
                        continue

                with self._lock:
                    self.in_use_sandboxes.add(sandbox_id)
                    if creation_time:
                        age_minutes = (time.time() - creation_time) / 60
                        self.logger.debug(f"Acquired sandbox {sandbox_id} (age: {age_minutes:.1f}m)")

                return sandbox_id

        except asyncio.TimeoutError:
            raise TimeoutError(f"Failed to acquire sandbox within {timeout}s timeout")

    async def release(self, sandbox_id: str):
        """
        Release a sandbox back to the pool for immediate reuse.

        Note: Cleanup disabled - bundles use unique names so no conflicts.
        """
        with self._lock:
            self.in_use_sandboxes.discard(sandbox_id)

            if sandbox_id not in self.all_sandboxes:
                self.logger.error(f"Attempted to release unknown sandbox {sandbox_id}")
                return

            creation_time = self.sandbox_creation_times.get(sandbox_id)

        if creation_time:
            age_seconds = time.time() - creation_time
            remaining_seconds = self.timeout_seconds - age_seconds
            age_minutes = age_seconds / 60

            if remaining_seconds < self.timeout_seconds * 0.2:
                self.logger.info(
                    f"Sandbox {sandbox_id} nearing timeout (age: {age_minutes:.1f}m, {remaining_seconds / 60:.1f}m remaining), removing from pool"
                )
                await self.remove(sandbox_id)
                return

        try:
            self.ready_queue.put_nowait(sandbox_id)
        except queue.Full:
            # Queue full - this should NEVER happen if producer is working correctly
            self.logger.error(
                f"CRITICAL: Pool queue full ({self.ready_queue.qsize()}/{self.pool_size}), "
                f"deleting sandbox {sandbox_id}. This indicates a logic error!"
            )
            with self._lock:
                self.all_sandboxes.discard(sandbox_id)
            try:
                await self.sandbox_client.delete(sandbox_id)
            except Exception:
                pass
        except Exception as e:
            self.logger.error(f"Failed to return {sandbox_id} to pool: {repr(e)}")
            traceback.print_exc()
            # If we can't return it, remove from tracking
            with self._lock:
                self.all_sandboxes.discard(sandbox_id)
            try:
                await self.sandbox_client.delete(sandbox_id)
            except Exception:
                pass

    async def remove(self, sandbox_id: str):
        """
        Remove a dead/failed sandbox from the pool without returning it to the queue.
        The producer will automatically create a replacement.
        """
        with self._lock:
            self.in_use_sandboxes.discard(sandbox_id)
            self.all_sandboxes.discard(sandbox_id)
            creation_time = self.sandbox_creation_times.pop(sandbox_id, None)

        age_minutes = (time.time() - creation_time) / 60 if creation_time else None
        age_str = f" (age: {age_minutes:.1f}m)" if age_minutes is not None else ""
        self.logger.warning(f"Removed dead sandbox {sandbox_id}{age_str} from pool, producer will create replacement")

    async def shutdown(self):
        """Shutdown the producer thread and clean up all sandboxes."""
        self.logger.info("Shutting down sandbox pool...")

        # Signal producer thread to stop
        self.shutdown_event.set()

        # Wait for producer thread to exit
        if self.producer_thread is not None and self.producer_thread.is_alive():
            self.logger.debug("Waiting for producer thread to exit...")
            self.producer_thread.join(timeout=10.0)
            if self.producer_thread.is_alive():
                self.logger.warning("Producer thread did not exit cleanly")

        # Delete all sandboxes using bulk_delete
        with self._lock:
            all_ids = list(self.all_sandboxes)

        if all_ids:
            self.logger.info(f"Cleaning up {len(all_ids)} sandboxes via bulk_delete...")
            try:
                await self.sandbox_client.bulk_delete(sandbox_ids=all_ids)
                self.logger.info(f"Successfully deleted {len(all_ids)} sandboxes")
            except Exception as e:
                self.logger.warning(f"Error bulk deleting sandboxes: {repr(e)}")

        with self._lock:
            self.all_sandboxes.clear()
            self.in_use_sandboxes.clear()

        self.logger.info("Sandbox pool shutdown complete")
